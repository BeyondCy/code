文档名称：内核冥思(Think In Kernel)
文档维护：Xuefeng Chang(welfear@gmail.com)
文档日期：2010.8.22



零、概论：


经典操作系统理论通常将内核分成进程、内存、文件系统、输入/输出四个部分。
Linux Kernel的目录结构正是这样，它主要由mm、kernel、fs、ipc组成。
NT Kernel的目录结构稍稍不同，它主要是由mm、ke、io、ex、ps、cc组成。
其中mm是最为独立的，Linux中的Kernel实际上对应了NT中的ex、ke、ps，而NT
统一了Linux中的fs和ipc两部分，用ex提供的机制在io中实现，而这里所谓的
“实现”也只是提供了一个框架而已。排除硬件抽象层（Linux的arch和NT的Hal），
Linux和NT的内核设计观点并不相同，Linux坚守传统，NT思想前卫。Linux努力
把一切当作文件，而NT试图把一切当作对象。在内核内部，Linux只为module提供了
Kernel API使用，NT却为Driver提供了Kernel API和Native API。NT统一了数据
交换机制，而Linux则分而治之。当然，分或合各有好处。总的来说，
ex + ke + ps == kernel, ex + io == vfs + ipc。ex实现了对象管理和各种执行体
对象，前者统一了对象访问方式，后者统一了对象使用策略。更有意思的是两者
对fs和io的设计上：Linux通过fs框架实现io，NT通过io框架实现fs。本文仅限于
Linux和NT两个操作系统，体会NT的设计之美和品味Linux的实现之美。文中提到的
版本号只是大概并一定准确。至于GUI应该放在Kernel还是作为一个Process提供
或哪种内核网络栈实现更优秀都不在本文讨论范围内。


一、中断：


中断是最重要的内核活动，内核借助中断实现有优先级的内核路径执行和内核任务的
随机检测。中断路径往往都比较短，复杂的任务往往会交给内核线程或工作队列完成。
这两者在Linux中是分开的，程序员在觉得可能需要检查任务的时候，便会主动检查，
在需要提高当前线程优先级时便会禁止相应的可能会执行的内核路径。NT的中断系统
通过IRQL的概念将两者融入在一起，如果没有硬件支持IRQL，NT会通过软件模拟。
它的重要原则是IRQL提升时，内核路径的优先级也会提升；IRQL下降时，在优先级下降
的同时，系统会由大到小检查IRQL经过的内核路径是否有任务需要执行。这个过程是
简单的电平触发，合理的设置内核路径以及它们的优先级可以让它们按优先级得到
运行机会。

广义上的中断是指硬件中断和软件中断两种。硬件中断大体对应各种设备的中断服务
程序，虽然每个处理器都可以设置自己的IDT，但由于设备是计算机全局资源，所以
硬件中断表几乎是一致的。软件中断有针对当前CPU和当前线程两种。

N.B. NT的时钟中断是个例外，负责更新系统时间的主CPU时钟中断服务程序是特殊的。

由于硬件中断程序不能长时间运行（优先级高对系统的影响就大），所以软件中断需要
完成部分硬件相关的任务。Linux中就是softirq，NT中则是DPC，它们都是与特定CPU
相关的。NT DPC还会检测是否需要执行调度器，而Linux在中断返回、系统调用/异常
返回时检测是否需要执行调度器。

N.B. 为了提高网络性能，Linux加入了Google的RPS/RPF Patch，该补丁会Linux的
软中断实现负载均衡。NT NDIS并不依赖DPC而是workqueue处理网络数据包，因此NT
的DPC不会成为性能瓶颈。不过，这项技术依然值得借鉴，它使软中断任务不必依次
进行，提高了并发效率。

FIXME. DPCThread.

N.B. 实时NT方案和RT-Linux很像，都是接管硬件响应，将通用的操作系统和实时模块
分开调度。<Inside Windows>中第三方针对实时性扩展NT的例子：
http://www.venturcom.com/pdfs/RTXWhitePaper-6-09.pdf

另一种针对特定线程软中断，包括Linux的Singal和NT的APC。其典型应用是异步I/O、
异常处理等。信号机制很简单，它在系统调用和异常处理返回时获得执行机会，除了
SIGUSR，其他信号并不排队也不累加。NT的APC机制则要复杂得多，而APC本身就分成
三种：User APC、Special Kernel APC、Normal Kernel APC。用户APC和信号相似，
它会在系统调用和系统异常返回时得到运行，并返回到一个用户空间地址中。Linux中
是应用程序设置的信号处理函数地址，而NT则是ntdll.dll的KeUserApcDispatcher。
特殊内核APC比普通内核模式APC拥有更高的优先级。下面是区分方法：

Special APC: NormalRoutine  NULL KernelRoutine !NULL
Normal APC:  NormalRoutine !NULL KernelRoutine !NULL

特殊APC可抢占普通APC，特殊APC只运行KernelRoutine，而普通APC先运行KernelRoutine
后运行NormalRoutine。Kernel APC可以通过IRQL变化时经过APC_LEVEL时得到运行，或在
切换线程上下文时，根据调度需求运行Kernel APC。特殊APC常用在I/O完成、结束进程等
场合，它不会再次发起I/O请求。根据OSR文档<Asynchronous Procedure Calls in NT>，
普通APC有可能再次发起I/O请求，并且DispatchRead, DispatchWrite, DispatchCtrl可以
在IRQL = APC_LEVEL时运行在任意线程上下文。这意味着在使用不依赖IRQL机制同步数据
访问时需要禁止普通APC以避免发生死锁。文件系统驱动常常需要禁止Normal Kernel APC，
因为它往往会使用内核执行体资源锁，该锁借助事件和信号量在低IRQL环境实现读写锁，
在当前线程捕获锁后，如果普通APC抢占执行并再次发起I/O请求，那么它有可能再次捕获
该锁，造成死锁。

N.B. 理论上普通APC会再次发起I/O，但这种情况很少发生。笔者只知道在第一次I/O请求
发生错误时，文件系统会发起普通APC，这可能会查询文件名并重试一次。

NT中禁止普通APC(KeEnterCriticalRegion)和Linux禁止抢占原理类似，内核只需要线程
控制块中增加或减少锁计数即可，因为同一个线程不能同时运行在两个CPU上。禁止
特殊APC需要借助IRQL或用KeEnterGuardedRegion(NT5x)。


二、调度：


NT的调度基于传统的时间片和优先级操作系统理论，而Linux的调度比较复杂，甚至
有点怪。Linux是逐渐发展的，这包括调度器的完善和抢占的支持，前者削弱了时间片
概念，后者体现了优先级概念。即便如此NT与Linux的调度看起来还是那么不同，比如
线程状态，Linux中的RUNNING对应于NT中的就绪，NT中的运行状态则被Linux放在了
处理器控制块中，因为它们是一一对应的关系。又比如抢占，它增加了编程的复杂性，
因为同步问题是最难发现的一类问题，然而Linux早期并没有考虑抢占，所以Linux的
抢占并没有NT中那么“强硬”，Linux会在几个合适的时机检测是否应该抢占，或者
内核认为当前线程运行了足够长的时间，应该检测是否被抢占了。

N.B. 对于开发者来说，BSOD(Blue Screen Of Death)是很好的系统特性，然而对用户
来说未必。Linux支持oops和panic两种方案。调试器应该是除错的辅助手段而不应该
完全依靠它，在内核发现错误时立即停止运行将有助于除错。

优先级：

NT中存在32个优先级，从低到高为0~31，但0级保留给零页系统线程（每个CPU都有
一个为内存页清零的线程）。1~15级用于非实时线程，16~31级用于实时线程。

线程优先级由进程优先级类和线程相对优先级类两者决定。进程优先级类从高到低
有五类：实时、高、普通上、普通、普通下、空闲。线程相对优先级类从高到低有
七类：时间关键、最高、普通上、普通、普通下、最低、空闲。

对于非实时线程来说，时间关键和空闲分别对应于15级和1级；对于实时线程来说，
它们分别对应31级和16级。其它五类为线程对应进程的进程优先级类的相对值，
因此每个进程优先级类有5级，它们的中间值（也就是普通类）分别是：
24,13,10,8,6,4。确定了进程优先级类和线程相对优先级类后，一个线程的优先级便
随之确定。

NT简单的参考优先级调度线程，高优先级优先运行。它会根据动态改变非实时线程的
优先级，而从不改变实时线程的优先级。同一优先级的线程公平的轮转时间片运行，
这一般发生在实时线程中，非实时线程的优先级经常发生改变。被抢占的线程会获得
特殊的同情，它会被放在最前面。

Linux中140个优先级，从低到高为139~0，0~99保留给实时线程，100~139保留给非实时
线程，nice的范围是-20~19。nice值和非实时线程的优先级相差120，通过修改nice就
可以达到修改线程优先级的目的。

时间片：

对于x86平台，UP的时钟间隔大约是10ms，MP的时钟间隔大约是15ms。客户系统上，
线程默认运行2个时钟间隔，服务器系统上，线程默认运行12个时钟间隔。只有这两种
选择（短时间片和长时间片），可通过“我的电脑->属性->高级->性能选项”改变
设置。“程序”代表默认2，“后台服务”代表12。最可能是Windows中最让人莫名其妙
的设置选项了。前台线程也可能会受到特殊照顾，在短时间片系统上，前台窗口对应的
线程可以运行6个时钟间隔。长时间片系统上默认不改变前台线程的时钟间隔。上述
提到的设置都可以在注册表的Win32PrioritySeparation中修改，比如长或短时钟
间隔、前台线程是否改变时间片、改变多少等。

状态：

线程可以是CPU密集型或I/O密集型。两者主要通过等待时间多少加以区分的，后者
往往需要等待键盘、鼠标、网络数据包或软件事件。等待结束时，当前线程需要及时
响应，NT会提高线程优先级达到这个目的，至于提高多少，则是由该等待对象本身
决定的。总的来说，运行、等待、就绪是线程三个主要状态。

1.谦让：

yield会使当前线程放弃运行，然而出乎意料的是它可能失败。如果当前CPU上没有
其他就绪线程，那么它只能继续运行。

2.等待：

wait会使满足条件的线程进入等待状态。如果使用自旋锁无法锁住数据，那么很有
可能是该线程在自旋锁内进入了等待状态。如果需要等待，系统会将当前线程放入
等待对象的等待队列中，如果有必要还会为当前线程设置timer。系统会在时钟中断
触发时检查timer是否到时。每个CPU都有自己的IDT，时钟中断会在毫秒级别上发出
中断，每个CPU的时钟中断会错开发生，主CPU负责更新系统时间。

3.强迫：

线程会被抢占或是被重新调度。抢占发生在调度锁解锁之时，重新调度发生在时钟
中断发现线程时间片用完之时。调度锁只是一般的自旋锁，然而在解锁时，它会检查
当前CPU控制块，确定是否发生了抢占，如有必要，它会引起一次调度。改变线程的
状态或优先级都可以引起系统抢占当前线程，空闲线程会在一定时间内循环查看是否
有新线程抢占。

N.B. idle：Linux不断调用pause指令，NT是用sti, hlt。理论上NT的CPU温度更低。
NT4在SMP系统上只用sti，它可以引发直到中断打开的延迟。NT4 SMP没有用
hlt是因为hlt引发snoop机制，降低性能，而XP SMP用的是hlt。

处理器：

NT中有两个处理器参数和调度有关：亲缘性和完美处理器。亲缘性是硬限制，该线程
必须运行在一个特定的CPU集合中，完美处理器是软限制，该线程最好运行在此处理器
上。这两个参数可以用API设置，另外还有一个运行参数：上次运行的处理器，这是
基于处理器高速缓存命中率的考虑。调度时系统会在处理器亲缘性集合中寻找满足
完美处理器或上次运行的处理器。Linux只有亲缘性概念。

N.B. 调度时还有一个考虑因素是就绪线程的等待时间。如果一个就绪线程在3个Tick
之内还没有被调度运行，那么系统会优先使它运行。然而NT5.1之前存在BUG，直到
NT5.1，这个规则才算实现。

算法：

NT5x之前的调度算法并不复杂，它主要是基于一个按优先级划分的就绪线程队列。
一个线程会因为状态的变化而进入或离开队列，而一个线程在时间片用完之时也需要
为当前CPU需要下一个要运行的线程。NT保存了空闲处理器和就绪队列的位图加快调度
速度，下面是一些典型的情况：

1.一个等待线程满足条件加入就绪队列。系统在唤醒一个线程时会根据等待对象要求
提高该线程的优先级（它下降时是每用完时间片降低一个优先级）。这时，系统优先
考虑亲缘性范围内的空间CPU，否则它试图抢占当前正在运行线程，如果它的优先级
真的比较高，那就真的将自己设置在当前处理器控制块中，稍后调度锁解锁时，NT会
发生调度。这个被抢占的“倒霉蛋”会被设置为抢占，并被放入就绪队列头部。

2.如果线程抢占没有成功，那么它只好等待一次次等待时钟中断的到来。NT按优先级
和每个优先级对应链表的先后顺序调度。在查找同级别线程时，NT会试图在列表中找
相对于当前CPU具有更好参数的线程。

3.平衡管理器会扫描4秒钟内还没有得到运行的“倒霉蛋”。这个过程也是通过提高
优先级实现的。平衡管理器机制用于防止系统中出现优先级反转问题。

NT5x优化了锁性能和调度效率，很明显，全局的就绪队列会成为系统的hotspot。在
NT5x中，全局的就绪队列就分别进入到每个处理器控制块中，分别加解锁即可。
每个处理器的空闲线程需要扫描其它处理器的就绪线程，以填补自己的“肌饿”。这
也会发生在一个线程需要等待而暂时离开就绪队列时，因为CPU“肌饿”最有可能发生
在这种情况下。
NT6的调度使用rdtsc指令作为时间片的单位实现一种更公平的调度。它会提高临界锁
性能，因为它大大提高了临界锁解锁时发生调度的可能性，这会有效的减少锁竞争。
NT6的新调度机制DFSS(dynamic fair share scheduling)可以通过修改注册表中的
HKLM\SYSTEM\CurrentControlSet\ControlSession\ManagerQuota\System关闭。

N.B. Intel 3B TIME-STAMP COUNTER: Intel CPU的TSC有的产品按固定频率增长，
有的按实际频率增长(AMD亦如此)。

FIXME. NUMA

Linux使用由nice值转换的优先级，此外该优先级值还可以当作默认的I/O优先级值，
应用程序需要使用util-linux库中的ioprio_set修改I/O优先级。I/O调度优先级需要
块设备使用的I/O调度算法的支持，比如CFQ。CFS调度器放弃了之前使用的复杂计算
线程运行时间经验公式，而根据优先级做为权重计算出的线程应该运行的“公平”
运行时间的多少。Linux中的就绪队列实际上是红黑树，树的键值便是线程的虚拟运行
时间。CFS调度器实际上是按照线程虚拟运行时间与实际时间的比值调度。Linux假设在
一段时间内，如果每个线程都得到公平的对待，那么Linux只需要让比值最小的线程
运行即可。

N.B. NT和Linux在进程运行时间统计上有类似的方法。它们都是按Tick为单位，
用采样的方法计算运行时间。Tick有可能落在User, Interrupt, Dpc, Kernel
几种环境。IdleTime就是Idle的Kernel。不过发生在si, hi的概率很小，一旦
发生补偿也大。也就是说：process + idle + si + hi = 100%。

N.B. NT6加入了WorkerFactory类型的对象和Dpc执行管理，但线程池的基本原理
与以前版本相同，都是基于Timer和APC实现。


三、输入输出：


模式：


涉及I/O的应用程序中往往存在两部分：I/O事件分配器、I/O事件处理器。

1.Reactor:

I/O事件处理器发送I/O请求，事件分配器等待I/O发生，将等待的I/O事件分配给
事件处理器，后者完成I/O事件并处理后续任务。

2.Proactor:

I/O事件处理器发送I/O请求，事件分配器等待I/O发生，并完成I/O。事件处理器
仅仅完成I/O后续处理任务。

两者似乎区别不大，无非是由谁来完成I/O。别着急，我们慢慢向下看。


模型：


要实现上文提到的两种模式，我们还需要为应用程序选择一种程序工作模型。

1.兵来将挡型：

每次有新的I/O发生时，我们都为它新创建一个线程或进程，处理完再销毁。

在Linux上创建新进程会借助COW机制，NT上一般会选择创建线程。但这种方案还是
会让人从直觉上认为它太懒又太慢。

N.B. NT在创建进程时也会使用COW机制，但相比于Linux的fork模型，它只是部分
使用此技术。

2.客随主便型：

一个或多个I/O线程或进程循环等待I/O事件，事件来到后，将I/O分配给一个或多个
工作线程或进程。

这个方案有两点改进，一是用了多路复用技术，二是用池技术。多路复用使用广泛，
与前面同步I/O不同，它是一种半异步I/O。它将I/O等待统一在一起，但多了分配
I/O过程。

不要担心惊群问题，那是太古老的现象了。经过排列组合后，模型有很多种，我们可
以根据情况玩出很多花样。有多少种积木供我们选择呢？


接口：

Linux的I/O模型可以简单按照阻塞或非阻塞、同步或异步进行如下划分：

read/write:     block, synchronous
O_NONBLOCK:     non-block, synchronous
select/poll:    block, asynchronous
aio_read/write: non-block, asynchronous

NT的I/O模型是异步模型为基础，同步请求是通过等待异步信号实现的。Linux将这些
复杂的I/O模型交给文件系统模块实现，因此，有的模型可能没有实现，比如aio。
NT的I/O模型其实不能按照是否阻塞和是否同步划分，比如同步的语意并不是同步数据
而是同步I/O请求。Overlapped非空时代表异步请求，否则语意是Linux下的同步。

N.B. FileObject只有一个信号，所有同步操作会等待同一个信号，相互制约。
使用scatter/gather IO似乎只有数据库软件，它需要直接读写文件。
Linux的aio_xxx只有使用O_DIRECT打开的文件才能直接向块设备发送I/O请求，而
Windows要异步和无缓存两个条件才会相同的效果。

1.select:

它会在特定时间内监视读、写、异常三组文件描述符的状态。每组的文件描述符有
数目限制，Linux设置为1024，这和默认的进程打开文件描述符限制相同。Linux内核
对select的数目没有限制，最大为当前线程已经打开的文件数。所以想要突破1024的
限制可以重新编译libc，或强制转换fd_set数据结构，输入更多的fd，或是直接调用
sys_select或sys_pselect。NT中与select类似的是WaitForXxxObject，winsock2接口
提供了select。由于NT文件系统没有规定poll的接口，因此select的实现是NT网络
协议栈驱动通过ioctl实现的，它只受理0xFFFF个文件。与Linux类似，用poll实现
select。select的接口是其低效的原因之一，它只说明了一个最大数目，而不是三个
组分别有多少文件，如果各组之间的文件数相差太多时，内核需要测试大量无效文件
描述符。值得一提的是pselect可以用做高精度定时器。

N.B. Winsock2提供了WSAEnumNetworkEvents，它改良了select接口并针对网络应用
增加了些功能，比如在connect或accept时，它会返回FD_WRITE事件。

2.poll:

准确的说，poll只是改良了select的参数。Linux文件系统支持poll，select的实现
借用了poll的调用返回信息。poll的改变其实很简单，用一个数组代替了三个。从这
一点上说，poll与WaitForXxxObject也有点像，更像的是它们的算法。poll测试文件
状态，测试的过程并不相同。Linux的poll会调用文件系统的poll，而NT统一用相同
的数据结构，因此测试工作由它自己完成。从这里我们能看出它们设计思想的不同，
Linux的poll可实现更多功能，但如果文件对应的文件系统没有实现poll，那么只能
忽略它。NT上poll的行为更加统一，它基于一个简单的开关模型。在没有发现满足
条件的文件时，poll会使当前线程进入等待状态。当线程被唤醒后（可能timer到期，
也可能是有I/O事件），它会再次测试数组中的文件，并为它们设置I/O状态。返回
后，应用程序很有可能会再次测试数组内文件的状态。poll虽然没有了文件数量上的
限制，但它依然低效，select和poll可能会因为每次调用都要复制文件描述符数组而
浪费时间。

3.epoll:

epoll又改良了poll。在接口上，epoll不再是一个系统调用，而扩展为一组系统调用。
这意味着，Linux会组织专门的数据管理这些需要等待I/O事件的文件，因此没有文件
描述符数组复制来复制去了。epoll_wait每次只是返回一个满足条件的文件描述符，
因此epoll也不用测试每个文件的状态了。它需要找到一个即可，从这一点上看，它和
WaitForXxxObject相似。根据活动文件数和文件总数之比考虑两种极端情况，比值
越大poll的效率越高，比值越低epoll的效率越高。epoll还可以设置电平触发或边沿
触发两种模式，而select, poll, WaitForXxxObject都是电平触发，边沿触发会提高
触发准确性，提高效率。而其它机制只能暂时将文件描述符设置失效的方法避免再次
触发。

Vietor Liu: epoll支持两种触发模式：LT(水平模式)、ET（边缘模式）。LT模式是
默认的，与select与poll的行为一样，即事件未处理完将重复触发事件。ET模式则
对于事件只触发一次，事件处理完才能触发下一次，应用程序处理不当容易造成事件
丢失。ET模式是高效应用的基础，单次触发模式能够有效提升句柄检测效率。文件
句柄关闭时，epoll中的注册信息被自动删除，在网络应用中可以使用shutdown来
触发EPOLLRDHUP事件来实现优雅关闭。

4.WaitForXxxObject:

对比上述三种设计，WaitForXxxObject显然是一种简单与效率的折中选择。它只支持
64个等待对象，并只指出一个满足条件的对象。MSDN上说明了两种扩展等待对象数目
的方法，RegisterWaitForSingleObject和等待等待线程（不是笔误）。它能等待所有
能打开的对象。但等待不常见的对象可能导致出人意料的行为，因为每个对象都有
自己的规则，它们在不同的状态会有不同等待结果。WaitForXxxObject使用一个固定
长度的对象数组，每次只找一个满足条件的对象；进入等待状态直到等待条件满足，
它不会再次测试每个对象状态，它可以直接获得满足条件对象的序号。

FIXME. SignalObjectAndWait

5.iocp:

什么才是更完美的I/O模型？既简单又高效。我们考虑整个过程，应该避免如下开销：
等待I/O、线程切换、并发线程数高、内存复制、CPU竞争、I/O响应慢、锁竞争等。
每次poll需要三次线程切换（接收、发送、分配工作线程），每次epoll需两次线程
切换（接收、分配），它们都会有一次滞后的同步读和一次可能阻塞的同步写。还要
如何改进？半异步模型中，接收、发送和分配是三种最消耗时间的过程。由于I/O请求
都是同步完成，那么接收和发送两个过程必然会导致线程切换（发送指等待接收），
而管理线程池也需要一次。而异步非阻塞模型可以避免接收和发送的两次切换，实际的
读写工作也可以提前完成，工作线程只需要关心完成的结果。这种模式也有不足之处，
它使所有I/O请求都处于异步完成状态，然而有些请求是可以同步完成的，这可能会使
IOCP在同步完成时会发生一次线程切换。之所以说可能是因为存在工作线程发送并由
它自己再次接收完成结果的可能，也就是没有等待线程时。这在并发线程数大于等于
实际线程数的情况下发送几率更高。
IOCP其实只是两个个简单链表，准确说只有一个。I/O完成时，该请求会加入到发出I/O
请求线程对应的Port链表中，另一方面，空闲线程查询该Port是否有需要处理的请求，
如果没有线程会等待该Port对象。由于对象的等待链表就是由对象头结构维护的，因此
该等待线程会被加到Port对象头中的链表。
那么其它的开销是如何避免的呢？IOCP默认的并发数是当前活动CPU个数；同一进程的
线程会分配到不同的CPU调度；在I/O事件发生时，线程会提高优先级（提高值由设备
驱动决定）；使用Interlock锁避免使用临界区可以使多线程并发运行效率更高。

N.B. 对于异步Winsock，用setsockopt将SO_SNDBUF设置为0可以禁止Nagle算法，减少
一次内存复制，这在某些情况下会提高性能。一般来说，网络数据会经过三次复制，
分别是从网卡复制到驱动，从驱动复制到协议栈缓存，从缓存复制到用户提供的缓冲区。

6.aio:

Linux没有IOCP该怎么办？aio可以完成异步I/O；信号量可用于管理线程池，并控制
并发数，它可以简单模拟Port，当然还需要一个锁保护的待处理I/O链表；再为线程
设置CPU亲缘性、优先级和IO响应优先级；NPTL锁替代interlock；美中不足的是aio
使用signal机制，而NT异步使用Kernel APC机制，理论上APC响应更快，但这应该
不会有太大影响。

N.B. Linux aio的异步并不是真正的异步。glibc实现的aio是用pthread创建一个
I/O等待线程，并用pread/pwrite同步读写。而Linux的libaio库可以直接使用内核
提供的io_submit等系统调用，然而这组API依然是同步模式，只不过由用户线程换成
内核workqueue。也就是说Linux实质上还是基于同步轮询模式，这也会带来上述提到
的各种开销问题。NT的异步模型相比于Linux的同步模型多了完成机制，如果没有完成
步骤，驱动程序难以完成复杂的异步模型。

N.B. Linux aio的行为和相应的内核模块实现有关。对于O_DIRECT打开的块设备来说，
它会调用aio_complete完成aio请求，然而网络驱动却没有实现，它只能依靠轮询。

FIXME. grep -Inr -C10 aio_complete linux-2.6


四、内存：


内存管理器是内核中最基础的部分，它必须满足系统中其它组件对各种用法和用途的
内存需求。更有意思的是，按照Ln速度和容量的分级理论，内存处在高速处理器缓存
和慢速外部设备之间，它对系统的效率和性能的影响都是不容忽视的。
主板上的内存并不全是可用的，低处有640KB内存存在“历史遗留问题”，而高处还要
为各种设备和ACPI预留部分地址。内核通过调用BIOS得到计算机内存的e820图得到相关
信息（BIOS厂商会给计算机生厂商预留修改BIOS的接口）。物理内存需要通过MMU映射
为虚拟地址后才可以使用。一般Linux中内核态的物理地址+0xC0000000便是虚拟地址，
这个简化也影响了Linux的内存管理算法。内存管理的算法很多，但基本上还是使用
位图和链表两种基本的数据结构。这些算法需要在速度、外碎片、内碎片、内存利用率
之间作出取舍，它们的思想早在上世纪六十年代便已产生，Doug Lea在八十年代末写出
用于早期glibc的dlmalloc，Jeff Bonwick在九十年代初实现了Slab算法。请参考：
<Dynamic Storage Allocation: A Survey and Critical Review>

N.B. 文件系统存在相似的情形，它们同样常用bitmap和list，复杂的文件系统会用
B-Tree或B+ Tree。例如FAT文件系统，它是Bill Gates和Marc McDonald在1976年开发的，
其核心思想FAT来源于简单的链表数组。

批发：

Linux用Buddy System算法管理页级内存批发工作。伙伴算法速度很快(O(logN))，
但可能产生的内碎片也容易很大（它只能分配页数为2的幂次）。

N.B. Buddy System是指Binary Buddy System。伙伴系统有很多种，最常见的依次
按2的幂次大小组成的二进制伙伴系统。

NT使用一种改良的first-fit策略的空闲链表算法(freelist)，降低了速度而减少
碎片。页分配算法有一定的特殊限制，它的分配大小固定，并且需要首地址按页
大小对齐。常用于堆内存分配算法，比如first-fit/best-fit，需要在分配内存块
之前放入一些管理数据，而页分配中不允许这样做。

N.B. 比如在Windows中，malloc返回地址减16字节处是堆块实际分配的内存大小，
malloc返回地址减14字节处便是它前一个块的实际内存大小，如果它是0，那么
malloc返回地址减12字节处一定是1。如果修改这些管理数据，那么最好的结果
恐怕就是程序马上崩溃掉，尽管堆可能会用0xbeadbeef之类的方法检查一下。

页分配算法更需要考虑内存回收时的合并方法，这关系到碎片的数目。伙伴算法巧妙
的解决了这个问题，NT使用的freelist算法只能在空闲页首放置管理数据，而页回收
时，它如何得知前后页状态呢？这个问题可以简单理解为NT在页对应的PTE保留位中
设置了分配信息，释放时如果NT发现相邻的页都是空闲，那么它会利用空闲页首的
数据合并相邻的内存块。

N.B. NT registry也用空闲链表管理注册表的Key和Value。它和页分配类似，需要
分配固定大小并按块大小对齐的块，所以它没有空间放置块前的管理结构，又由于
它的大小不固定也没有bitmap，所以它只有扩展和查找操作，没有合并操作。

在分配页时，NT考虑了分配页对处理器高速缓存的影响，也就是使用PageColor技术。
现代处理器按物理地址缓存，这就有可能会使虚拟地址相邻页同时竞争相同的缓存。
Linux没有PageColor机制，而FreeBSD、Solaris10都支持。不支持的理由很简单，
现在主流处理器Cache都是关联的，所以性能提升并不大，而且实现起来也复杂，
用虚拟地址Cache的处理器没这个问题。Linux使用的Slab算法考虑了Cache性能因素。
NT将内存分成两部分，一部分保留给内核使用，另一部分按需分配。保留给内核的
内存又分成两部分，一部分组成分页内存池，另一部分组成非分页内存。内核会
按照系统内存总数计算它们的大小，一般为几十兆，典型值为48MB。分页内存池用
bitmap管理，bitmap速度比较慢(O(N))。非分页内存可以被扩展，用freelist管理。
按需分配的内存用list管理，它支持页的加入和剔除。

N.B. 非分页内存池和分页内存池的算法并不相同。分页内存池的内存可能被分页，
因此空闲页首页不能放置管理结构。只好使用简单并有效的位图管理大小固定的
分页内存池，而管理分页内存池的bitmap是放在非分页内存池中。

Linux和NT的内存管理有一个重要的区别，Linux管理的是物理地址连续的内存，而
NT管理的是虚拟地址连续的内存。简单的说，NT利用MMU机制减少内存碎片，这会
频繁操作页表而使CPU同步TLB，另一方面，分配不连续的物理内存会降低TLB命中率，
两者都会影响系统性能。因此，频繁使用的内存应该在非分页内存中分配。

FIXME. Linux hugetlbpage. 有些处理器支持巨型TLB，这项技术可提高TLB命中率。

如果某个该死的驱动要求很多连续的内存怎么办？因为非分页内存池中的内存是
建立时就已经完成映射的，因此内核函数MmAllocateContiguousMemory会先去搜索
这个区域，如果没找到，那么它将会暴力搜索其它内存分配区域，这可能是个漫长
的过程。扩展的分页内存池只是为了满足某些临时没有合适内存块分配的情况，它
需要先找到合适系统PTE，再一页页设置好映射关系。

N.B. NT5对freelist进行了优化，将单一的freelist分成1页、2页、3页、多于3页
4组空闲链表加快查找速度，并针对单页内存另设置了缓冲列表(smallest-first,
best-fit order)。

零售：

NT中存在若干种类的内存分配策略，它们的原理大致相同，并都取自上述的非分页
内存池和分页内存池。NT的“零售”算法修改自伙伴系统，这种算法同样应用在
进程堆，只不过进程堆需要实现更多的特性和功能。系统中经常进行的交易是零售，
NT中，这两种行为是按页大小划分的。NT为每种内存策略都设置了8个空闲链表用于
小于页大小的内存分配；对于小于512字节的内存分配，也有8个链表，但不是
freelist，而是lookaside。

N.B. NT5开始小于512字节的内存分配并不从统一链表上获取，而是从当前CPU控制块
中获取，这显然是希望提高CPU Cache的命中率。同样，小内存块的最小分配单位是
32字节也是为了防止多CPU Cache的false sharing问题。

与伙伴系统不同的是，NT按照倍数而不是指数安排空闲列表。小内存列表按照32字节
的倍数排列，而大内存列表按照512字节的倍数排列。分配的内存块前包含8字节的
管理数据。lookaside用于相同内存块的分配，它是一种包含深度信息的单链表，
而freelist中的内存块大小则并不固定，但内存块的大小有范围限制。

Linux参考了Solaris的Slab算法管理零售行为，Linux中存在三种相似的算法：slab、
slub、slob。Slab实际上有Cache和Slab两种管理数据结构，Slab优化特定大小的数据
分配。系统中经常需要大量特定大小的数据结构，每种Cache管理一种特定内存大小的
分配，Cache用三个链表（满、半满、闲）管理Slab，每个Slab包含若干个该特定大小
的内存块，Slab用数组管理内存块。Slab中数组的个数与特定内存块的大小有关，大小
没有限制。Slab算法对特定结构管理得快又好，但对通用的内存分配就没那么完美。
Linux为一般内存分配函数kmalloc从32字节开始按2幂指数设置若干个Cache。kmalloc
有大小限制，但至少可以分配16个页，更多内存需要用vmalloc分配，vmalloc分配的
内存是虚拟地址连续的。对kmalloc使用不当可能会造成过多的内存碎片。

进程：

内核按页为进程分配所需内存。这种分配是被动的，内核只是为进程设置PTE，但不
真正分配内存页，等到进程真正访问时内核会在PageFault异常中满足进程的要求。
NT5x之前使用Splay Tree管理地址空间，NT5x之后用AVL Tree管理地址空间，而Linux
用Red-Black Tree。Linux将进程的虚拟地址空间管理和文件映射接口统一在一起，堆
分配函数brk()是通过无文件参数的mmap()实现的。mprotect()用于设置映射页的
页属性，但没有API获取属性，这些信息需要通过分析/proc/self/maps文件获得。NT里
相似的工作由Section、VirtualMemory、FileSystem三个部分协作完成的。

N.B. Linux与NT对用户线程栈的处理并不相同。Linux的堆是从低到高扩展，栈是从高
到低扩展，而NT中堆和栈都是从低到高扩展。从这个角度上看，在Linux中突破线程栈
大小更容易实现。

N.B. NT的MMU使用了自映射方案，PDT和PTE可以由同一PTE表示，因此每个进程可以
节省一页内存。该技术详见Intel编程手册。

传输：

应用程序和内核之间需要相互复制内存，这是因为内核并不信任用户空间的内存。
对于少量内存，比如系统调用的参数，Linux处理得很小心，它会在内核栈中复制
一份，NT则是利用它在异常处理方面的优势大胆得访问进程空间的内存。但对于大量
内存来说，比如文件读写缓冲区，内存复制就有可能产生性能问题。NT提供了避免
内存复制的内存使用方式，内核需要解决任意进程上下文的问题。当有数据来临时，
驱动程序不能假定当前进程空间就是发起请求的进程，因此驱动程序可能不得已又
进行了一次复制内存。NT会先刺探内存是否有效，刺探内存是比较快速的过程，这
只需要按页访问内存并捕获可能产生的异常。然后NT会锁住PFN，防止进程删除映射。
最后再次将用户态内存映射至内核地址空间，新地址便可供驱动程序使用。

N.B. NT系统调用接口设计有可能会引起TOCTTOU(Time-of-check-to-time-of-use)的
问题，比如用指针接收Handle，NT先刺探进程空间内存是否会有异常。成功后会对象
创建再写句柄值，但这时可能会引发异常，但NT忽略异常，返回成功，而对象也就
无法关闭了。虽然这样的问题几乎不会发生，利用起来也十分困难，但这确实是一个
竞争问题。Linux系统调用多是通过寄存器传递的，所以没有这一逻辑问题。

驱动程序可以设置内核处理进程内存的使用方式：可以是从内存池中分配一份内核
缓冲区，也可以是创建内存描述符，并锁住、重新映射内存。Linux aio实现也用到
了类似的机制，它将进程页映射到系统高端内存，这样可以在任意进程空间上下文
读写这些内存了。


FIXME.
五、文件：

文件映射：

文件缓存：

预读算法：
